# Decision Tree Classifier from Scratch  This project is a Python implementation of a Decision Tree classifier built from scratch using only NumPy and standard Python libraries. It's designed to demonstrate the core concepts of decision tree algorithms, including entropy, information gain, and recursive splitting.  The implementation is contained within a single Jupyter Notebook, which also includes an example of how to train and test the classifier on the Scikit-learn breast cancer dataset.  ## Features    * **Pure Python/NumPy:** The core classifier logic is built from the ground up.   * **Information Gain:** Uses **entropy** to calculate **information gain** for finding the optimal split at each node.   * **Hyperparameter Control:** Allows tuning of key parameters:       * `max_depth`: The maximum depth of the tree.       * `min_samples_split`: The minimum number of samples required to split an internal node.       * `n_features`: The number of features to consider when looking for the best split (useful for feature subsampling).   * **Classification:** Designed for binary or multi-class classification tasks.  ## How to Run  1.  **Clone the repository:**     ```bash     git clone <your-repository-url>     cd <your-repository-name>     ``` 2.  **Install dependencies:**     The project requires `numpy` for calculations and `scikit-learn` for loading the example dataset and splitting the data.     ```bash     pip install numpy scikit-learn jupyterlab     ``` 3.  **Run the notebook:**     Start Jupyter Lab and open the `decisiontreeimplementation (1).ipynb` notebook.     ```bash     jupyter lab     ```     You can then run all the cells to see the classifier build, train, and achieve an accuracy of \~93% on the breast cancer test set.  ## How It Works  The implementation is broken into two main classes: `Node` and `DecisionTree`.  ### `Node` Class  This is a simple helper class that represents a single node in the tree.    * A **decision node** has a `feature`, `threshold`, `left` child, and `right` child.   * A **leaf node** has a `value`, which represents the final predicted class.  ### `DecisionTree` Class  This is the main classifier with the following core methods:    * `fit(X, y)`: The public method called to train the model. It initializes the recursive `_grow_tree` process.    * `_grow_tree(X, y, depth)`: This is the recursive function that builds the tree. It stops (creates a leaf node) if any of an's stopping criteria are met:      1.  The `max_depth` is reached.     2.  The number of samples at the node is less than `min_samples_split`.     3.  The node is "pure" (all samples belong to the same class).    * `_best_split(X, y, feat_idxs)`: This method finds the best feature and threshold to split the data on. It iterates through a random subset of features (`feat_idxs`) and all unique values within those features. For each combination, it calculates the **Information Gain** and selects the split that maximizes it.    * `_information_gain(y, X_column, threshold)`: Calculates the Information Gain (IG) of a potential split. IG measures the reduction in entropy.      $$     $$$$IG = E\_{\\text{parent}} - \\left( \\frac{n\_{\\text{left}}}{n} E\_{\\text{left}} + \\frac{n\_{\\text{right}}}{n} E\_{\\text{right}} \\right)      $$     $$$$Where $E$ is the entropy and $n$ is the number of samples.    * `_entropy(y)`: Calculates the entropy of a set of labels, which is a measure of impurity.      $$     $$$$E(y) = - \\sum\_{i} p\_i \\log\_2(p\_i)      $$     $$$$Where $p_i$ is the probability of class $i$ in the node. (Note: The code uses `np.log` (natural log), which works just as well as $\log_2$ since it's a constant factor difference).    * `predict(X)`: Takes a set of new samples and, for each one, uses the `_traverse_tree` helper function to navigate from the root to a leaf node to determine the predicted class.  ## Example Usage  (This code is included in the notebook)  ```python from sklearn import datasets from sklearn.model_selection import train_test_split import numpy as np  # --- Assume Node and DecisionTree classes are defined above ---  # 1. Load data data = datasets.load_breast_cancer() X, y = data.data , data.target  # 2. Split data X_train , X_test , y_train , y_test = train_test_split(     X, y, test_size=0.2, random_state=1234 )  # 3. Initialize and train the classifier clf = DecisionTree(max_depth=10) clf.fit(X_train, y_train)  # 4. Make predictions y_pred = clf.predict(X_test)  # 5. Calculate accuracy def accuracy(y_true, y_pred):   return np.sum(y_true == y_pred) / len(y_true)  acc = accuracy(y_test, y_pred) print(f"Accuracy: {acc:.4f}") # Output: Accuracy: 0.9298 ```  ## License  This project is licensed under the MIT License.
